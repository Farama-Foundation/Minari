import html
import logging
import os
import pathlib
import re
import warnings
from collections import defaultdict
from typing import OrderedDict

import yaml
from md_utils import dict_to_table

import minari
from minari.dataset.minari_dataset import gen_dataset_id, parse_dataset_id
from minari.utils import get_dataset_spec_dict
from gen_dataset_md import (
    NO_ENV_MSG,
    NO_TRAIN_ENV_MSG,
    NO_EVAL_ENV_MSG,
    PRE_TRAIN_ENV_MSG,
    PRE_EVAL_ENV_MSG,
)


logger = logging.getLogger(__name__)

DATASET_FOLDER = pathlib.Path(__file__).parent.parent.joinpath("datasets")


def _rst_escape(text: str) -> str:
    return text.replace("*", r"\*").replace("_", r"\_").replace("`", r"\`")


def _html_escape(text: str) -> str:
    return html.escape(text, quote=True)


def _short_desc(desc: str) -> str:
    """Extract the first sentence from a description."""
    match = re.search(r"\.(?!\s*(?:Mr|Mrs|Dr|Ms|Prof|Sr|Jr)\b)(?:\s*[A-Z]|\n)", desc)
    return (desc[: match.start() + 1] if match else desc).split("\n", 1)[0]


def _parse_dataset_group(dataset_group: str) -> tuple:
    """Parse a dataset group path into remote_path and prefix.

    Args:
        dataset_group: Full path to dataset group (e.g., "hf://username/group-name")

    Returns:
        Tuple of (remote_path, prefix) where:
        - remote_path is the remote storage path (e.g., "hf://username")
        - prefix is the namespace/group within that remote (e.g., "group-name")
    """
    if "://" in dataset_group:
        remote_type, rest = dataset_group.split("://", maxsplit=1)
        parts = rest.split("/", maxsplit=1)
        remote_name = parts[0]
        prefix = parts[1] if len(parts) > 1 else None
        remote_path = f"{remote_type}://{remote_name}"
        return remote_path, prefix
    return None, dataset_group


def _get_group_name_from_dataset_group(dataset_group: str) -> str:
    """Extract a filesystem-safe group name from a dataset group path.

    Args:
        dataset_group: Full path to dataset group (e.g., "hf://username/group-name")

    Returns:
        Group name suitable for use as folder name (e.g., "group-name")
    """
    if "://" in dataset_group:
        _, rest = dataset_group.split("://", maxsplit=1)
        parts = rest.split("/", maxsplit=1)
        # Return just the prefix part (the group name), not the remote user
        return parts[1] if len(parts) > 1 else parts[0]
    return dataset_group


def _generate_dataset_page(
    dataset_id: str,
    metadata: dict,
    group_folder: pathlib.Path,
    load_path: str,
):
    """Generate a page for an individual dataset."""
    namespace, dataset_name, version = parse_dataset_id(dataset_id)
    versioned_name = gen_dataset_id(None, dataset_name, version)

    content = "---\nautogenerated:\n"
    content += f"title: {dataset_name.title()}\n"
    content += "---\n\n"
    content += f"# {dataset_name.title()}"
    content += "\n\n"

    if "description" in metadata:
        content += "## Description"
        content += "\n\n"
        content += metadata["description"]
        content += "\n\n"

    content += "## Dataset Specs"
    content += "\n\n"
    content += dict_to_table(get_dataset_spec_dict(metadata))
    content += "\n\n"

    # Environment specs section
    env_spec = metadata.get("env_spec")
    eval_env_spec = metadata.get("eval_env_spec")

    if env_spec is None and eval_env_spec is None:
        content += NO_ENV_MSG
    else:
        content += "\n## Environment Specs\n"
        if env_spec is None:
            content += NO_TRAIN_ENV_MSG
        else:
            content += PRE_TRAIN_ENV_MSG.format(load_path)
            content += "\n"

        content += """\n## Evaluation Environment Specs\n"""
        if eval_env_spec is None:
            content += NO_EVAL_ENV_MSG.format(load_path)
        else:
            content += PRE_EVAL_ENV_MSG.format(load_path)
            content += "\n"

    # Write dataset page in the correct namespace folder
    if namespace:
        dataset_folder = group_folder.joinpath(namespace)
    else:
        dataset_folder = group_folder
    os.makedirs(dataset_folder, exist_ok=True)

    dataset_md_path = dataset_folder.joinpath(f"{versioned_name}.md")
    with open(dataset_md_path, "w", encoding="utf-8") as f:
            f.write(content)

    return versioned_name


def _generate_namespace_page(namespace_path: pathlib.Path, title: str, namespace_content: dict):
    """Generate an index page for a namespace (matching gen_dataset_md.py style)."""
    file_content = "---\nfirstpage:\nlastpage:\n---\n\n"
    file_content += f"# {title}\n\n"

    file_content += "## Content\n"
    file_content += "|     ID     | Description |\n"
    file_content += "| ---------- | ----------- |\n"
    for c in sorted(namespace_content.values(), key=lambda x: x["display_name"]):
        file_content += f'| <a href="{c["file"]}" title="{c["display_name"]}">{c["display_name"]}</a> | {c["short_description"]} |\n'

    file_content += "\n```{toctree}\n:hidden:\n"
    for c in namespace_content.values():
        file_content += f"{c['toctree']}\n"
    file_content += "```\n"

    namespace_file = namespace_path.joinpath("index.md")
    with open(namespace_file, "w", encoding="utf-8") as f:
        f.write(file_content)


def _generate_dataset_group_pages(group_entry):
    """Generate pages for a dataset group with proper nested namespace handling."""
    dataset_group = group_entry.get("dataset_group", "")
    display_name = group_entry.get("display_name", "")

    if not dataset_group:
        warnings.warn(f"Skipping group entry without dataset_group: {group_entry}")
        return None

    remote_path, prefix = _parse_dataset_group(dataset_group)
    group_name = _get_group_name_from_dataset_group(dataset_group)

    try:
        # List all datasets in this group
        remote_datasets = minari.list_remote_datasets(
            remote_path=remote_path, prefix=prefix, latest_version=True
        )

        if not remote_datasets:
            warnings.warn(f"No datasets found in {dataset_group}")
            return None

        # Create group folder under community/
        group_folder = DATASET_FOLDER.joinpath("community", group_name)
        os.makedirs(group_folder, exist_ok=True)

        # Build namespace contents structure (like gen_dataset_md.py)
        namespace_contents = defaultdict(OrderedDict)

        for dataset_id, metadata in remote_datasets.items():
            namespace, dataset_name, version = parse_dataset_id(dataset_id)

            # Generate dataset page
            load_path = f"{remote_path}/{dataset_id}"
            _generate_dataset_page(dataset_id, metadata, group_folder, load_path)

            # Build namespace hierarchy
            if namespace is not None:
                # Create folder structure for namespace
                group_folder.joinpath(namespace).mkdir(parents=True, exist_ok=True)

                # Add sub-namespaces to their parent's content
                ns_parts = namespace.split("/")
                for i in range(1, len(ns_parts)):
                    parent = "/".join(ns_parts[:i])
                    sub_namespace = "/".join(ns_parts[: i + 1])
                    if sub_namespace not in namespace_contents[parent]:
                        namespace_contents[parent][sub_namespace] = {
                            "short_description": "",
                            "file": ns_parts[i],
                            "toctree": f"{ns_parts[i]}/index",
                            "display_name": ns_parts[i][0].upper() + ns_parts[i][1:],
                        }

                # Add dataset to its namespace's content
                versioned_name = gen_dataset_id(None, dataset_name, version)
                namespace_contents[namespace][dataset_id] = {
                    "short_description": _short_desc(metadata.get("description", "")),
                    "file": versioned_name,
                    "toctree": versioned_name,
                    "display_name": versioned_name,
                }
            else:
                # Dataset at root level of the group
                versioned_name = gen_dataset_id(None, dataset_name, version)
                namespace_contents[""][dataset_id] = {
                    "short_description": _short_desc(metadata.get("description", "")),
                    "file": versioned_name,
                    "toctree": versioned_name,
                    "display_name": versioned_name,
                }

        # Generate namespace index pages for all namespaces
        for namespace, content in namespace_contents.items():
            if namespace == "":
                # Root level - this becomes the group index
                namespace_path = group_folder
                title = display_name
            else:
                namespace_path = group_folder.joinpath(namespace)
                # Use the last part of namespace as title
                title = namespace.split("/")[-1]
                title = title[0].upper() + title[1:]

            _generate_namespace_page(namespace_path, title, content)

        # If there's no root content but there are namespaces, create a root index
        if "" not in namespace_contents and namespace_contents:
            # Find top-level namespaces
            top_level = {}
            for ns in namespace_contents.keys():
                if "/" not in ns:
                    top_level[ns] = {
                        "short_description": "",
                        "file": ns,
                        "toctree": f"{ns}/index",
                        "display_name": ns[0].upper() + ns[1:],
                    }
            if top_level:
                _generate_namespace_page(group_folder, display_name, top_level)

        logger.info(f"Generated community dataset group page for {group_name} ({len(remote_datasets)} datasets)")
        return group_name, len(remote_datasets)

    except Exception:
        logger.exception(f"Failed to generate page for {dataset_group}")
        return None


def _get_group_info(group_entry) -> dict:
    """Get info for a dataset group.

    Args:
        group_entry: Group entry from YAML

    Returns:
        Dict with group info including dataset count
    """
    dataset_group = group_entry.get("dataset_group", "")
    remote_path, prefix = _parse_dataset_group(dataset_group)
    try:
        remote_datasets = minari.list_remote_datasets(
            remote_path=remote_path, prefix=prefix, latest_version=True
        )
        return {
            "display_name": group_entry.get("display_name", ""),
            "dataset_count": len(remote_datasets),
        }
    except Exception:
        return {
            "display_name": group_entry.get("display_name", ""),
            "dataset_count": 0,
        }


def generate_community_page(
    yaml_path=DATASET_FOLDER.joinpath("community", "community.yaml"),
    out_md=DATASET_FOLDER.joinpath("community", "index.md"),
):
    if not os.path.exists(yaml_path):
        raise FileNotFoundError(f"YAML file not found: {yaml_path}")

    with open(yaml_path) as f:
        community_data = yaml.safe_load(f) or []

    # Collect group info for toctree
    group_entries_info = []
    for group_entry in community_data:
        result = _generate_dataset_group_pages(group_entry)
        if result:
            group_name, dataset_count = result
            group_entries_info.append({
                "group_name": group_name,
                "display_name": group_entry.get("display_name", group_name),
                "dataset_count": dataset_count,
            })

    # Generate the index page with cards (using MyST markdown)
    content = "---\nfirstpage:\nlastpage:\n---\n\n"
    content += "# Community Datasets\n\n"
    content += "Below is a list of dataset groups contributed by the community.\n\n"

    # Cards using raw HTML (same style as before)
    content += "```{raw} html\n"
    content += '<div class="sphx-glr-thumbnails">\n'
    content += "```\n\n"

    for info in sorted(group_entries_info, key=lambda x: x["display_name"]):
        display_name = _html_escape(info["display_name"])
        group_name = info["group_name"]
        dataset_count = info["dataset_count"]
        local_url = f"/datasets/community/{group_name}/"

        content += "```{raw} html\n"
        content += f'<div class="sphx-glr-thumbcontainer" tooltip="{display_name} ({dataset_count} datasets)" style="min-height: 120px;">\n'
        content += f'<a href="{local_url}"><img src="/_static/img/minari-text.png" alt="{display_name}" style="width: 250px; height: auto; display: block; margin: 0 auto;"></a>\n'
        content += f'  <div class="sphx-glr-thumbnail-title">{display_name}</div>\n'
        content += "</div>\n"
        content += "```\n\n"

    # Add "Add your own dataset" card
    readme_url = "https://github.com/Farama-Foundation/Minari/blob/main/docs/datasets/community/README.md"

    content += "```{raw} html\n"
    content += f'<div class="sphx-glr-thumbcontainer" tooltip="Add your own dataset group to this page" style="min-height: 120px;">\n'
    content += f'<a href="{readme_url}"><img src="/_static/img/minari-text.png" alt="Add your dataset here" style="width: 250px; height: auto; display: block; margin: 0 auto;"></a>\n'
    content += '  <div class="sphx-glr-thumbnail-title">Add your dataset here</div>\n'
    content += "</div>\n"
    content += "```\n\n"

    # Close thumbnails div
    content += "```{raw} html\n"
    content += "</div>\n"
    content += "```\n\n"

    # Toctree for sidebar navigation (makes groups collapsible in sidebar)
    content += "```{toctree}\n:hidden:\n\n"
    for info in group_entries_info:
        content += f"{info['group_name']}/index\n"
    content += "```\n"

    os.makedirs(os.path.dirname(out_md), exist_ok=True)
    with open(out_md, "w", encoding="utf-8") as f:
        f.write(content)


def main():
    generate_community_page()


if __name__ == "__main__":
    main()
