{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Behavioral cloning with PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We present here how to perform behavioral cloning on a Minari dataset using [PyTorch](https://pytorch.org/).\nWe will start generating the dataset of the expert policy for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, which is a classic control problem.\nThe objective is to balance the pole on the cart, and we receive a reward of +1 for each successful step.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\nFor this tutorial you will need the [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) library, which you can install with `pip install rl_zoo3`.\nLet's then import all the required packages and set the random seed for reproducibility:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport sys\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom gymnasium import spaces\nfrom rl_zoo3.train import train\nfrom stable_baselines3 import PPO\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport minari\nfrom minari import DataCollector\n\n\ntorch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy training\nNow we can train the expert policy using RL Baselines3 Zoo.\nWe train a PPO agent on the environment:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sys.argv = [\"python\", \"--algo\", \"ppo\", \"--env\", \"CartPole-v1\"]\ntrain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will generate a new folder named `log` with the expert policy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset generation\nNow let's generate the dataset using the [DataCollector](https://minari.farama.org/api/data_collector/) wrapper:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = DataCollector(gym.make('CartPole-v1'))\npath = os.path.abspath('') + '/logs/ppo/CartPole-v1_1/best_model'\nagent = PPO.load(path)\n\ntotal_episodes = 1_000\nfor i in tqdm(range(total_episodes)):\n    obs, _ = env.reset(seed=42)\n    while True:\n        action, _ = agent.predict(obs)\n        obs, rew, terminated, truncated, info = env.step(action)\n\n        if terminated or truncated:\n            break\n\ndataset = env.create_dataset(\n    dataset_id=\"cartpole/expert-v0\",\n    algorithm_name=\"ExpertPolicy\",\n    code_permalink=\"https://minari.farama.org/tutorials/behavioral_cloning\",\n    author=\"Farama\",\n    author_email=\"contact@farama.org\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once executing the script, the dataset will be saved on your disk. You can display the list of datasets with ``minari list local`` command.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Behavioral cloning with PyTorch\nNow we can use PyTorch to learn the policy from the offline dataset.\nLet's define the policy network:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this scenario, the output dimension will be two, as previously mentioned. As for the input dimension, it will be four, corresponding to the observation space of ``CartPole-v1``.\nOur next step is to load the dataset and set up the training loop. The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\nHowever, since each episode can have a varying length, we need to pad them.\nTo achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n    return {\n        \"id\": torch.Tensor([x.id for x in batch]),\n        \"observations\": torch.nn.utils.rnn.pad_sequence(\n            [torch.as_tensor(x.observations) for x in batch],\n            batch_first=True\n        ),\n        \"actions\": torch.nn.utils.rnn.pad_sequence(\n            [torch.as_tensor(x.actions) for x in batch],\n            batch_first=True\n        ),\n        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n            [torch.as_tensor(x.rewards) for x in batch],\n            batch_first=True\n        ),\n        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n            [torch.as_tensor(x.terminations) for x in batch],\n            batch_first=True\n        ),\n        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n            [torch.as_tensor(x.truncations) for x in batch],\n            batch_first=True\n        )\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now proceed to load the data and create the training loop.\nTo begin, let's initialize the DataLoader, neural network, optimizer, and loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minari_dataset = minari.load_dataset(\"cartpole/expert-v0\")\ndataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n\nenv = minari_dataset.recover_environment()\nobservation_space = env.observation_space\naction_space = env.action_space\nassert isinstance(observation_space, spaces.Box)\nassert isinstance(action_space, spaces.Discrete)\n\npolicy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\noptimizer = torch.optim.Adam(policy_net.parameters())\nloss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the cross-entropy loss like a classic classification task, as the action space is discrete.\nWe then train the policy to predict the actions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 32\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        a_pred = policy_net(batch['observations'][:, :-1])\n        a_hat = F.one_hot(batch[\"actions\"].type(torch.int64))\n        loss = loss_fn(a_pred, a_hat.type(torch.float32))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, we can evaluate if the policy learned from the expert!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\nobs, _ = env.reset(seed=42)\ndone = False\naccumulated_rew = 0\nwhile not done:\n    action = policy_net(torch.Tensor(obs)).argmax()\n    obs, reward, terminated, truncated, _ = env.step(action.numpy())\n    done = terminated or truncated\n    accumulated_rew += reward\n\nenv.close()\nprint(\"Accumulated rew: \", accumulated_rew)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visually observe that the learned policy aces this simple control task, and we get the maximum reward 500, as the episode is truncated after 500 steps.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}